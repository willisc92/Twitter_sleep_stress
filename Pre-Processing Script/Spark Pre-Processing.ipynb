{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3 # Try with 10\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly, try with 5120 later\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"1:00\" #1 hour\n",
    "os.environ['SBATCH_PARTITION']='lattice' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cheunw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import emoji\n",
    "from datetime import datetime, timedelta\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "import re\n",
    "import ssl\n",
    "import certifi\n",
    "from time import sleep\n",
    "import nltk \n",
    "from nltk import bigrams \n",
    "from nltk.util import ngrams\n",
    "import sys\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, file_name):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(file_name, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "\n",
    "def parseTimeZone(latitude, longitude):  # Get timezone object from latitude, longitude\n",
    "    tf = TimezoneFinder()\n",
    "    timezone = tf.timezone_at(lat=latitude, lng=longitude)\n",
    "    if timezone is None:\n",
    "        timezone = tf.closest_timezone_at(lat=latitude, lng=longitude)\n",
    "    return timezone\n",
    "\n",
    "def parseDate(date_info, latitude,\n",
    "              longitude):  # Method to parse the date/time of a tweet into the tweeter's local time.  \n",
    "    orig_date = datetime.strptime(date_info, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "    fmt = '%a, %b %d %Y %H:%M:%S'\n",
    "    new_date = datetime.strptime(datetime.strftime(orig_date, fmt), fmt)\n",
    "    tz = parseTimeZone(latitude, longitude)\n",
    "    localized_time = new_date.astimezone(pytz.timezone(tz))\n",
    "    offset = int(str(localized_time)[-6:-3])\n",
    "    delta = timedelta(hours=offset)\n",
    "    localized_time = new_date + delta\n",
    "    return localized_time\n",
    "\n",
    "def parseProvince(city_province_string):  # Parse province from city_province_string\n",
    "    city_and_province = city_province_string.split(',')\n",
    "    if len(city_and_province) > 1:\n",
    "        return city_and_province[1].strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def getAccountLife(tweet_post_date, user_creation_date):  # Get the account life to the post in days.\n",
    "    tweet_date = datetime.strptime(tweet_post_date, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "    user_c_date = datetime.strptime(user_creation_date, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "    elapsed_time = tweet_date - user_c_date\n",
    "    elapsed_time_in_days = elapsed_time / timedelta(minutes=1) / 60 / 24\n",
    "    return elapsed_time_in_days\n",
    "\n",
    "def parseHour(date_info, lattitude,\n",
    "              longitude):  # Get the hours from time object.  Convert to time object with parseDate and get the hour.  Return an integer.\n",
    "    try:\n",
    "        parsed_date_info = parseDate(date_info, lattitude, longitude)\n",
    "        hour = int(parsed_date_info.hour)\n",
    "        return hour\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "\n",
    "def parseWeekday(date_info, lattitude,\n",
    "                 longitude):  # Get the weekday from the time object. Convert to time object with parseDate and get the weekday.  Return as string.  \n",
    "    try:\n",
    "        parsed_date_info = parseDate(date_info, lattitude, longitude)\n",
    "        weekday = str(parsed_date_info.weekday())\n",
    "        return weekday\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "\n",
    "def parseDay(date_info, lattitude,\n",
    "             longitude):  # Get the weekday from the time object. Convert to time object with parseDate and get the weekday.  Return as string.  \n",
    "    try:\n",
    "        parsed_date_info = parseDate(date_info, lattitude, longitude)\n",
    "        weekday = str(parsed_date_info.day)\n",
    "        return weekday\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "\n",
    "def getRawCleanWords(\n",
    "        tweet_text):  # Method to get clean words from text (no punctuation, lowercase, etc). Used to check for sleep/stress keywords.\n",
    "    tweet_text = tweet_text + \" .\"\n",
    "    tweet_text = tweet_text.replace('&amp;', \" and \")\n",
    "    tweet_text = tweet_text.replace('+', \" \")\n",
    "    tweet_text = tweet_text.replace('=', \" \")\n",
    "    tweet_text = tweet_text.replace('\\n', \" \")\n",
    "    tweet_text = tweet_text.replace('@', \" AT_\")\n",
    "    tweet_text = tweet_text.replace('#', \" \")\n",
    "    tweet_text = tweet_text.replace('-', \" \")\n",
    "    tweet_text = tweet_text.replace('\\'', \"\")\n",
    "    tweet_text = add_space(tweet_text)\n",
    "    tweet_text = emoji.demojize(tweet_text)\n",
    "    clean_words_1 = []\n",
    "    for word in re.split(' +', tweet_text):\n",
    "        if word.startswith('https'):\n",
    "            continue\n",
    "        else:\n",
    "            word = word.lower()\n",
    "            word = word.replace(\"\\\\\", \" and \")\n",
    "            word = word.replace(\"/\", \" and \")\n",
    "            word = re.sub(r'[^a-z0-9\\s_]', ' ', word)\n",
    "            while (word.startswith(\" \") or word.endswith(\" \")):\n",
    "                word = word.strip()\n",
    "            clean_words_1.append(word)\n",
    "    clean_sentence_1 = ' '.join(clean_words_1)\n",
    "    clean_words_2 = re.split(' +', clean_sentence_1)\n",
    "    for word in clean_words_2:\n",
    "        while (word.startswith(\" \") or word.endswith(\" \")):\n",
    "            word = word.strip()\n",
    "        while (word.startswith(\"_\") or word.endswith(\"_\")):\n",
    "            word = word.strip(\"_\")\n",
    "    return (' '.join(clean_words_2))\n",
    "\n",
    "def hasSleepKeywords(clean_words):  # Method returns true/false for presence of sleep keywords in clean text.  \n",
    "    list_sleep_words = [\"bed\", \"bedtime\", \"hibernation\",\"slumber\",\"coma\",\"doze\" \"sleep\", \"sack\", \"insomnia\",\n",
    "                        \"dodo\", \"zzz\", \"siesta\", \"tired\", \"nosleep\", \"cantsleep\", \"exhausted\", \"sleepless\",\n",
    "                        \"hours\", \"awake\", \"late\", \"rest\", \"asleep\",\"slept\",\"sleeping\", \"sleepy\", \"asleep\",\n",
    "                        \"shuteye\", \"nap\", \"oclock\", \"melatonin\",\"caffeine\", \"coffee\", \"ambien\", \"zolpidem\",\n",
    "                        \"lunesta\", \"intermezzo\", \"trazadone\", \"eszopiclone\",\"zaleplon\"] \n",
    "    list_sleep_bigrams = [[\"pass\", \"out\"], [\"get\", \"up\"], [\"wake\", \"up\"], [\"long\",\"week\"], [\"long\",\"night\"],\n",
    "                          [\"close\", \"eyes\"], [\"long\",\"day\"], [\"late\",\"night\"], [\"nod\",\"off\"], [\"bad\", \"night\"]]\n",
    "    \n",
    "    list_clean_words = re.split(' +', clean_words)\n",
    "    \n",
    "    for word in list_clean_words:\n",
    "        if word in list_sleep_words:\n",
    "            return True\n",
    "\n",
    "    tweet_dict_bigrams = list(bigrams(list_clean_words))\n",
    "    for each_bigram in tweet_dict_bigrams:\n",
    "        for sleep_bigram in list_sleep_bigrams:\n",
    "            if sleep_bigram[0] == each_bigram[0] and sleep_bigram[1] == each_bigram[1]:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def hasStressKeywords(clean_words):  # Method returns true/false for presence of stress keywords in clean text.   \n",
    "    list_stress_words = [\"cold\", \"sick\", \"life\", \"stress\", \"school\", \"depression\", \"fucking\",\n",
    "                         \"stressor\", \"anxiety\", \"pressure\", \"depressed\", \"study\", \"heart\", \"pain\",\n",
    "                         \"stressful\", \"job\", \"hate\", \"shit\", \"fuck\", \"suffer\", \"die\", \"kill\"\n",
    "                         \"baby\", \"surgery\", \"therapy\"]  # List of stress keywords to compare tweet text to.\n",
    "    list_stress_bigrams = [[\"work\", \"today\"], [\"busy\", \"work\"], [\"dont\", \"feel\"], [\"years\", \"ago\"], [\"dont\", \"care\"],\n",
    "                           [\"long\", \"time\"],\n",
    "                           [\"mental\", \"health\"], [\"feel\", \"bad\"], [\"suicidal\", \"thoughts\"], [\"feel\", \"guilty\"],\n",
    "                           [\"hard\", \"time\"], [\"mental\", \"illness\"]]\n",
    "    # List of stress keywords to compare tweet text to.\n",
    "    list_clean_words = re.split(' +', clean_words)\n",
    "    for word in list_clean_words:\n",
    "        if word in list_stress_words:\n",
    "            return True\n",
    "\n",
    "    tweet_dict_bigrams = list(bigrams(list_clean_words))\n",
    "    for each_bigram in tweet_dict_bigrams:\n",
    "        for stress_bigram in list_stress_bigrams:\n",
    "            if stress_bigram[0] == each_bigram[0] and stress_bigram[1] == each_bigram[1]:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def hasSpam(clean_words):  #Method to check for spam words or bigrams.\n",
    "    list_spam_trigrams = [[\"in\", \"our\", \"bio\"],\n",
    "                          [\"click\", \"the\", \"link\"],\n",
    "                          [\"click\", \"to\", \"apply\"],\n",
    "                          [\"a\", \"great\", \"fit\"],\n",
    "                          [\"for\", \"rent\", \"on\"],\n",
    "                          [\"recommend\", \"anyone\", \"for\"],\n",
    "                          [\"were\", \"hiring\", \"in\"],\n",
    "                          [\"if\", \"youre\", \"looking\"],\n",
    "                          [\"fit\", \"for\", \"you\"],\n",
    "                          [\"interested\", \"in\", \"a\"],\n",
    "                          [\"apartments\", \"for\", \"rent\"],\n",
    "                          [\"score\", \"a\", \"job\"]]    \n",
    "    tokenize = nltk.word_tokenize(clean_words)\n",
    "    tweet_dict_trigrams = ngrams(tokenize, 3)\n",
    "    for each_trigram in tweet_dict_trigrams:\n",
    "        for spam_trigram in list_spam_trigrams:\n",
    "            if spam_trigram[0] == each_trigram[0] and spam_trigram[1] == each_trigram[1] and spam_trigram[2] == each_trigram[2]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def negation_sub(\n",
    "        text):  # Method to append NEG_ to every word after a negation word, up till the next punctuation mark.  \n",
    "    transformed = re.sub(r'\\b(?:not|no|never|aint|doesnt|havent|lacks|none|mightnt|shouldnt|'\n",
    "                         r'cannot|dont|neither|nor|mustnt|wasnt|cant|hadnt|isnt|neednt|without|'\n",
    "                         r'darent|hardly|lack|nothing|oughtnt|wouldnt|didnt|hasnt|lacking|nobody|'\n",
    "                         r'nowhere|shant)\\b[\\w\\s]+[.,:;!?]',\n",
    "                         lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)),\n",
    "                         text,\n",
    "                         flags=re.IGNORECASE)\n",
    "    return transformed\n",
    "\n",
    "def is_emoji(s):  # Method to check if a string is an emoji.\n",
    "    return s in emoji.UNICODE_EMOJI\n",
    "\n",
    "def add_space(text):  # Method to add a space between word-emoji pairs.\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        if is_emoji(char):\n",
    "            char = 'emoji_' + char + \" \"\n",
    "            result += ' '\n",
    "        result += char\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "def parseTweetText(tweet_text):  #Method removes punctuation, lowercase, emojis,\n",
    "    # and adds negation filter.    \n",
    "    tweet_text = tweet_text + \" .\"\n",
    "    tweet_text = tweet_text.replace('&amp;', \" and \")\n",
    "    tweet_text = tweet_text.replace('+', \" \")\n",
    "    tweet_text = tweet_text.replace('=', \" \")\n",
    "    tweet_text = tweet_text.replace('\\n', \" \")\n",
    "    tweet_text = tweet_text.replace('@', \" AT_\")\n",
    "    tweet_text = tweet_text.replace('#', \" \")\n",
    "    tweet_text = tweet_text.replace('-', \" \")\n",
    "    tweet_text = tweet_text.replace('\\'', \"\")\n",
    "    tweet_text = add_space(tweet_text)\n",
    "    tweet_text = emoji.demojize(tweet_text)\n",
    "    filtered_words_1 = []\n",
    "\n",
    "    for word in re.split(' +', tweet_text):\n",
    "        if word.startswith('https'):\n",
    "            continue\n",
    "        else:\n",
    "            word = word.lower()\n",
    "            word = word.replace(\"\\\\\", \" and \")\n",
    "            word = word.replace(\"/\", \" and \")\n",
    "            word = re.sub(r'[^a-z0-9\\s.,:;!?_]', '', word)\n",
    "            word = word.strip()\n",
    "            if (word != \"\"):\n",
    "                filtered_words_1.append(word)\n",
    "\n",
    "    filtered_sentence_1 = ' '.join(filtered_words_1)\n",
    "    filtered_words_2 = []\n",
    "    filtered_sentence_2 = negation_sub(filtered_sentence_1)\n",
    "    for word in re.split(' +', filtered_sentence_2):\n",
    "        word = re.sub(r'[.,:;!?]', ' ', word)\n",
    "        while (word.startswith(\" \") or word.endswith(\" \")):\n",
    "            word = word.strip()\n",
    "        while (word.startswith(\"_\") or word.endswith(\"_\")):\n",
    "            word = word.strip(\"_\")\n",
    "        if (word != \"\"):\n",
    "            filtered_words_2.append(word)\n",
    "    last_filtered = ' '.join(filtered_words_2)\n",
    "    last_filtered = last_filtered.replace(\"emoji_ \", \"emoji_\")\n",
    "    return last_filtered\n",
    "\n",
    "\n",
    "def remove_comma(orig_text):\n",
    "    orig_text = orig_text.replace(\"\\n\", \" \")\n",
    "    return orig_text.replace(\",\", \" \")\n",
    "\n",
    "def map_province(province):\n",
    "    incorrect_provinces = [\"Subd. C\", \"Toronto\", \"Subd. B\", \"Vancouver\", \"Subd. A\", \"Montréal\", \"Subd. D\", \"Calgary\",\n",
    "                           \"Subd. O\", \"Nouveau-Brunswick\"]\n",
    "    corrections = [\"Newfoundland and Labrador\", \"Ontario\", \"Nova Scotia\", \"British Columbia\",\n",
    "                   \"Newfoundland and Labrador\", \"Québec\", \"Newfoundland and Labrador\",\n",
    "                   \"Alberta\", \"Newfoundland and Labrador\", \"New Brunswick\"]\n",
    "    incorrect_correct_dict = dict(zip(incorrect_provinces, corrections))\n",
    "    if province in incorrect_provinces:\n",
    "        return incorrect_correct_dict[province]\n",
    "    else:\n",
    "        return province\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# sys.stdout = Logger(\"test_file_2_log_test_2.txt\")\n",
    "outputFileName = \"test_files_1_output_test.csv\"\n",
    "df = sqlCtx.read.json(\"/home/cheunw/Group Project Folder/Project Raw Data Files/1.json\")\n",
    "print(\"Number of tweets in dataframe\", df.count())\n",
    "# df.printSchema()\n",
    "\n",
    "desired_columns = (\n",
    "    expr(\"created_at as tweet_date\"),\n",
    "    expr(\"text as orig_text\"),\n",
    "    expr(\"user.id as user_id\"),\n",
    "    expr(\"user.verified as user_verified\"),\n",
    "    expr(\"user.followers_count as user_followers_count\"),\n",
    "    expr(\"user.friends_count as user_friends_count\"),\n",
    "    expr(\"user.listed_count as user_listed_count\"),\n",
    "    expr(\"user.statuses_count as user_statuses_count\"),\n",
    "    expr(\"user.created_at as user_creation_date\"),\n",
    "    expr(\"place.name as city\"),\n",
    "    expr(\"place.full_name as city_and_province\"),\n",
    "    expr(\"place.country as country\"),\n",
    "    expr(\"place.bounding_box.coordinates[0][0][0] as longitude\"),\n",
    "    expr(\"place.bounding_box.coordinates[0][0][1] as latitude\"),\n",
    "    \"lang\", \"is_quote_status\", \"retweeted\")\n",
    "\n",
    "content = df.select(*desired_columns)\n",
    "content = content.where(content.country == \"Canada\")\n",
    "content = content.where(content.lang == \"en\")\n",
    "content = content.where(content.retweeted == False)\n",
    "content = content.where(content.is_quote_status == False)\n",
    "\n",
    "#USER DEFINED FUNCTIONS WITH SPARK DATAFRAMES \n",
    "udf_remove_comma = udf(remove_comma, StringType())\n",
    "content = content.withColumn(\"orig_text\", udf_remove_comma(\"orig_text\"))\n",
    "\n",
    "#CHECK FOR SLEEP/STRESS \n",
    "udf_get_raw_clean_words = udf(getRawCleanWords, StringType())\n",
    "content = content.withColumn(\"raw_cleaned_text\", udf_get_raw_clean_words(\"orig_text\"))\n",
    "udf_get_sleep_marker = udf(hasSleepKeywords, BooleanType())\n",
    "content = content.withColumn(\"sleep_marker\", udf_get_sleep_marker(\"raw_cleaned_text\"))\n",
    "udf_get_stress_marker = udf(hasStressKeywords, BooleanType())\n",
    "content = content.withColumn(\"stress_marker\", udf_get_stress_marker(\"raw_cleaned_text\"))\n",
    "\n",
    "#FILTER ONLY SLEEP/STRESS\n",
    "content = content.where((content.sleep_marker == True) | (content.stress_marker == True))\n",
    "\n",
    "#Create SPAM/filter \n",
    "udf_spam_flag = udf(hasSpam, BooleanType())\n",
    "content = content.withColumn(\"spam_marker\", udf_spam_flag(\"raw_cleaned_text\"))\n",
    "\n",
    "#FILTER OUT SPAM\n",
    "content = content.where((content.spam_marker == False))\n",
    "\n",
    "#ADD CITY AND PROVINCE\n",
    "content = content.withColumn(\"city\", udf_remove_comma(\"city\"))\n",
    "udf_city_province_string_to_province = udf(parseProvince, StringType())\n",
    "content = content.withColumn(\"province\", udf_city_province_string_to_province(\"city_and_province\"))\n",
    "udf_map_provinces = udf(map_province, StringType())\n",
    "content = content.withColumn(\"province\", udf_map_provinces(\"province\"))\n",
    "\n",
    "#FILTER OUT PROVINCES \n",
    "content = content.where((content.province == \"Ontario\") |\n",
    "                        (content.province == \"Alberta\") |\n",
    "                        (content.province == \"British Columbia\") |\n",
    "                        (content.province == \"Québec\") |\n",
    "                        (content.province == \"Manitoba\") |\n",
    "                        (content.province == \"Yukon\") |\n",
    "                        (content.province == \"Nova Scotia\") |\n",
    "                        (content.province == \"Northwest Territories\") |\n",
    "                        (content.province == \"Newfoundland and Labrador\") |\n",
    "                        (content.province == \"New Brunswick\") |\n",
    "                        (content.province == \"Saskatchewan\") |\n",
    "                        (content.province == \"Prince Edward Island\"))\n",
    "\n",
    "# GET THE TWEET HOUR, WEEKDAY, AND DAY.  FILTER OUT IF NULL.  \n",
    "udf_get_tweet_hour = udf(parseHour, IntegerType())\n",
    "content = content.withColumn(\"tweet_hour\", udf_get_tweet_hour(\"tweet_date\", \"latitude\", \"longitude\"))\n",
    "\n",
    "udf_get_tweet_weekday = udf(parseWeekday, StringType())\n",
    "content = content.withColumn(\"tweet_weekday\", udf_get_tweet_weekday(\"tweet_date\", \"latitude\", \"longitude\"))\n",
    "\n",
    "udf_get_tweet_day = udf(parseDay, StringType())\n",
    "content = content.withColumn(\"tweet_day\", udf_get_tweet_day(\"tweet_date\", \"latitude\", \"longitude\"))\n",
    "\n",
    "content = content.where(content.tweet_hour.isNotNull())\n",
    "content = content.where(content.tweet_day.isNotNull())\n",
    "content = content.where(content.tweet_weekday.isNotNull())\n",
    "\n",
    "# FINALLY, GET FILTERED TEXT AND ACCOUNT LIFE FOR REMAINING ROWS OF DF\n",
    "udf_get_final_cleaned_tweet = udf(parseTweetText, StringType())\n",
    "content = content.withColumn(\"filtered_text\", udf_get_final_cleaned_tweet(\"orig_text\"))\n",
    "\n",
    "udf_get_account_life_days = udf(getAccountLife, DoubleType())\n",
    "content = content.withColumn(\"account_life_days\", udf_get_account_life_days(\"tweet_date\", \"user_creation_date\"))\n",
    "\n",
    "# print(\"Printing new schema\")\n",
    "content.printSchema()\n",
    "# content.show(3, False)\n",
    "\n",
    "\n",
    "# QUERY TESTS\n",
    "\n",
    "columns_to_keep_in_clean_data = (\"filtered_text\",\n",
    "                                 \"orig_text\",\n",
    "                                 \"raw_cleaned_text\",\n",
    "                                 \"account_life_days\",\n",
    "                                 \"latitude\", \"longitude\",\n",
    "                                 \"city\", \"province\", \"country\",\n",
    "                                 \"tweet_day\", \"tweet_hour\", \"tweet_weekday\", \"user_id\",\n",
    "                                 \"user_followers_count\", \"user_friends_count\", \"user_listed_count\",\n",
    "                                 \"user_statuses_count\",\n",
    "                                 \"user_verified\")\n",
    "\n",
    "# EXTRACT THE TWEETS WITH GOOD COLUMNS\n",
    "content = content.select(*columns_to_keep_in_clean_data)\n",
    "\n",
    "print(\"Number of extracted tweets:\", content.count())\n",
    "\n",
    "# WRITE THE TWEETS TO THE CSV \n",
    "# content.write.option(\"header\", \"false\").csv(outputFileName)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time of execution in seconds:\", end_time - start_time)\n",
    "print(\"Writing to\", outputFileName, \"complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
