{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import emoji\n",
    "from datetime import datetime, timedelta\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import ssl\n",
    "import certifi\n",
    "import geopy.geocoders\n",
    "from time import sleep\n",
    "from nltk import bigrams\n",
    "\n",
    "import pyspark\n",
    "\n",
    "ctx = ssl.create_default_context(cafile=certifi.where())\n",
    "geopy.geocoders.options.default_ssl_context = ctx\n",
    "\n",
    "\n",
    "class FilteredTweet:  # Class to store the clean Tweet data.\n",
    "    counter = 0\n",
    "\n",
    "    def __init__(self, tweet_dict):\n",
    "        self.index = None\n",
    "        localtime = parseDate(tweet_dict)\n",
    "        self.tweet_weekday = localtime.weekday()\n",
    "        self.tweet_day = localtime.day\n",
    "        self.tweet_hour = localtime.hour\n",
    "        self.orig_text = tweet_dict['text']\n",
    "        self.filtered_text = parseTweetText(tweet_dict)\n",
    "\n",
    "        self.user_ID = tweet_dict['user']['id']\n",
    "        self.user_verified = tweet_dict['user']['verified']\n",
    "        self.user_followers_count = tweet_dict['user']['followers_count']\n",
    "        self.user_friends_count = tweet_dict['user']['friends_count']\n",
    "        self.user_listed_count = tweet_dict['user']['listed_count']\n",
    "        self.user_statuses_count = tweet_dict['user']['statuses_count']\n",
    "\n",
    "        user_creation_time = datetime.strptime(tweet_dict['user']['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        self.user_creation_year = user_creation_time.year\n",
    "        self.user_creation_month = user_creation_time.month\n",
    "        self.user_creation_day = user_creation_time.day\n",
    "\n",
    "        self.place_name = tweet_dict['place']['name']\n",
    "\n",
    "        city_and_province = tweet_dict['place']['full_name'].split(\",\")\n",
    "        if len(city_and_province) > 1:\n",
    "            self.place_province = city_and_province[1].strip()\n",
    "        else:\n",
    "            self.place_province = None\n",
    "\n",
    "        self.place_country = tweet_dict['place']['country']\n",
    "        self.place_longitude = float(tweet_dict['place']['bounding_box']['coordinates'][0][0][0])\n",
    "        self.place_latitude = float(tweet_dict['place']['bounding_box']['coordinates'][0][0][1])\n",
    "\n",
    "        list_hashtags_dict = tweet_dict['entities']['hashtags']\n",
    "        if len(list_hashtags_dict) > 0:\n",
    "            list_hashtags = list()\n",
    "            for hashtag_dict in list_hashtags_dict:\n",
    "                text = hashtag_dict['text']\n",
    "                text = text.lower()\n",
    "                text = re.sub(r'[^a-z\\s]', '', text)\n",
    "                list_hashtags.append(text)\n",
    "            self.hashtags = ','.join(list_hashtags)\n",
    "        else:\n",
    "            self.hashtags = None\n",
    "\n",
    "    def __str__(self):  # Represent the tweet as a dictionary.\n",
    "        return str(self.__dict__)\n",
    "\n",
    "    def setProvince(self, recursion=0):\n",
    "        # Method to set the province using geopy library and latitude/longitude if it does not exist from twitter data.\n",
    "        # Will run recursively 10 times as geolocator may time out.\n",
    "        geolocator = Nominatim(user_agent=\"my_application\")\n",
    "        try:\n",
    "            location = geolocator.reverse(str(self.place_latitude) + \",\" + str(self.place_longitude), timeout=geopy.geocoders.base.DEFAULT_SENTINEL)\n",
    "            if 'address' in location.raw.keys():\n",
    "                if 'state' in location.raw['address'].keys():\n",
    "                    self.place_province = location.raw['address']['state']\n",
    "                    sleep(1)\n",
    "        except GeocoderTimedOut as e:\n",
    "            if recursion > 10:\n",
    "                return\n",
    "            else:\n",
    "                sleep(1)\n",
    "                self.setProvince(recursion=recursion + 1)\n",
    "\n",
    "    def increment(\n",
    "            self):  # Method will increment the counter and set the index.  Called when new clean tweet is written.\n",
    "        FilteredTweet.counter = FilteredTweet.counter + 1\n",
    "        self.index = FilteredTweet.counter\n",
    "\n",
    "\n",
    "def filterCountry(tweet_dict, country):  # Filter tweet object based on country.  Takes arguments of a standard tweet\n",
    "    # dictionary and country (as string).\n",
    "    if 'place' in tweet_dict.keys():\n",
    "        if tweet_dict['place'] is not None:\n",
    "            if 'country' in tweet_dict['place'].keys():\n",
    "                if tweet_dict['place']['country'] == country:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filterTrackable(tweet_dict):  # Filter tweet based on whether it is trackable or not.\n",
    "    if parseTimeZone(tweet_dict) is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filterLanguage(tweet_dict, language):  # Filter tweet object based on language.\n",
    "    if 'lang' in tweet_dict.keys():\n",
    "        if tweet_dict['lang'] is not None:\n",
    "            if tweet_dict['lang'] == language:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filterOriginal(tweet_dict):  # Method to filter out tweets that are quotes or re-tweets.\n",
    "    if 'is_quote_status' in tweet_dict.keys():\n",
    "        if tweet_dict['is_quote_status']:\n",
    "            return False\n",
    "    if tweet_dict['retweeted']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def parseDate(tweet_dict):  # Method to parse the date/time of a tweet into the tweeter's local time.\n",
    "    date_info = tweet_dict['created_at']\n",
    "    orig_date = datetime.strptime(date_info, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "    fmt = '%a, %b %d %Y %H:%M:%S'\n",
    "    new_date = datetime.strptime(datetime.strftime(orig_date, fmt), fmt)\n",
    "    tz = parseTimeZone(tweet_dict)\n",
    "    localized_time = new_date.astimezone(pytz.timezone(tz))\n",
    "    offset = int(str(localized_time)[-6:-3])\n",
    "    delta = timedelta(hours=offset)\n",
    "    localized_time = new_date + delta\n",
    "    return localized_time\n",
    "\n",
    "\n",
    "def parseTimeZone(tweet_dict):  # Method to retrieve the timezone based on the user coordinates.\n",
    "    tf = TimezoneFinder()\n",
    "    if 'place' in tweet_dict.keys():\n",
    "        if tweet_dict['place'] is not None:\n",
    "            if 'bounding_box' in tweet_dict['place'].keys():\n",
    "                if 'coordinates' in tweet_dict['place']['bounding_box'].keys():\n",
    "                    if len(tweet_dict['place']['bounding_box']['coordinates']) > 0:\n",
    "                        longitude = float(tweet_dict['place']['bounding_box']['coordinates'][0][0][0])\n",
    "                        latitude = float(tweet_dict['place']['bounding_box']['coordinates'][0][0][1])\n",
    "                        timezone = tf.timezone_at(lat=latitude, lng=longitude)\n",
    "                        if timezone is None:\n",
    "                            timezone = tf.closest_timezone_at(lat=latitude, lng=longitude)\n",
    "                        return timezone\n",
    "                    else:\n",
    "                        return None\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def filterTweet(tweet_dict, language, country):  # Apply language, original, country, and trackable filters to a tweet.\n",
    "    return (filterCountry(tweet_dict, country) and filterLanguage(tweet_dict, language)\n",
    "            and filterOriginal(tweet_dict) and filterTrackable(tweet_dict))\n",
    "\n",
    "\n",
    "def is_emoji(s):  # Method to check if a string is an emoji.\n",
    "    return s in emoji.UNICODE_EMOJI\n",
    "\n",
    "\n",
    "def add_space(text):  # Method to add a space between word-emoji pairs.\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        if is_emoji(char):\n",
    "            char += ' '\n",
    "            result += ' '\n",
    "        result += char\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "def parseTweetText(tweetDict):  # Method to demojize tweet text, URLs, remove punctuation, and move to lowercase.\n",
    "    tweet_text = tweetDict[\"text\"]\n",
    "    tweet_text = tweet_text.replace('\\n', \" \")\n",
    "    tweet_text = add_space(tweet_text)\n",
    "    tweet_text = emoji.demojize(tweet_text)\n",
    "    filtered_words_1 = []\n",
    "    for word in re.split(' +', tweet_text):\n",
    "        if word.startswith('https'):\n",
    "            continue\n",
    "        else:\n",
    "            word = word.lower()\n",
    "            word = re.sub(r'[^a-z0-9\\s.,:;!?_#@]', '', word)\n",
    "            word = word.strip()\n",
    "            filtered_words_1.append(word)\n",
    "    filtered_sentence_1 = ' '.join(filtered_words_1)\n",
    "    filtered_words_2 = []\n",
    "    filtered_sentence_2 = negation_sub(filtered_sentence_1)\n",
    "    for word in re.split(' +', filtered_sentence_2):\n",
    "        word = re.sub(r'[.,:;!?]', '', word)\n",
    "        word = re.sub(r'[#@]', ' ', word)\n",
    "        while (word.startswith(\" \") or word.endswith(\" \")):\n",
    "            word = word.strip()\n",
    "        while (word.startswith(\"_\") or word.endswith(\"_\")):\n",
    "            word = word.strip(\"_\")\n",
    "        if (word != \"\"):\n",
    "            filtered_words_2.append(word)\n",
    "    return ' '.join(filtered_words_2)\n",
    "\n",
    "\n",
    "def negation_sub(text):\n",
    "    transformed = re.sub(r'\\b(?:not|never|aint|doesnt|havent|lacks|none|mightnt|shouldnt|'\n",
    "                         r'cannot|dont|neither|nor|mustnt|wasnt|cant|hadnt|isnt|neednt|without|'\n",
    "                         r'darent|hardly|lack|nothing|oughtnt|wouldnt|didnt|hasnt|lacking|nobody|'\n",
    "                         r'nowhere|shant)\\b[\\w\\s]+[.,:;!?#@]',\n",
    "                         lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)),\n",
    "                         text,\n",
    "                         flags=re.IGNORECASE)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def checkSleepWords(tweet_dict, list_sleep_words,\n",
    "                    list_sleep_bigrams):  # Method to check for existence of sleep keywords/bigrams in filtered text.\n",
    "    # Takes tweet dictionary, a list of sleep words, and list of sleep bigrams to compare to.\n",
    "    tweet_dict_words = tweet_dict.filtered_text.split()\n",
    "    tweet_dict_bigrams = list(bigrams(tweet_dict_words))\n",
    "    for word in tweet_dict_words:\n",
    "        if word in list_sleep_words:\n",
    "            return True\n",
    "\n",
    "    for each_bigram in tweet_dict_bigrams:\n",
    "        for sleep_bigram in list_sleep_bigrams:\n",
    "            if each_bigram[0] == sleep_bigram[0] and each_bigram[1] == sleep_bigram[1]:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def checkStressWords(tweet_dict,\n",
    "                     list_stress_words):  # Method to check for existence of stress keywords in filtered text.\n",
    "    # Takes a tweet dictionary and a list of stress words to compare to.\n",
    "    tweet_dict_words = tweet_dict.filtered_text.split()\n",
    "    for word in tweet_dict_words:\n",
    "        if word in list_stress_words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def province_filter(\n",
    "        filtered_tweet):  # Method to correct the province field of a filtered tweet.  Overwrites the filtered_tweet\n",
    "    # passed to it.\n",
    "    list_to_remove = [\"Minnesota\", \"Michigan\", \"Iowa\", \"Alaska\"]\n",
    "    list_provinces_to_replace = [\"Subd. V\", \"Nouveau-Brunswick\", \"Washington\", \"Montana\", \"North Dakota\"]\n",
    "    list_replacements = [\"Newfoundland and Labrador\", \"New Brunswick\", \"Alberta\", \"Saskatchewan\", \"Manitoba\"]\n",
    "    replacement_dict = dict(zip(list_provinces_to_replace, list_replacements))\n",
    "\n",
    "    if filtered_tweet.place_province in replacement_dict.keys():\n",
    "        filtered_tweet.place_province = replacement_dict[filtered_tweet.place_province]\n",
    "    elif filtered_tweet.place_province in list_to_remove:\n",
    "        filtered_tweet = None\n",
    "    return filtered_tweet\n",
    "\n",
    "\n",
    "filename = \"Twitter_Dataset.json\"  # Name of .json dataset to pass.  Should have 1 tweet per line of file.\n",
    "df = spark.read.json(filename)\n",
    "df.show()\n",
    "# filename = \"E:/Machine Learning Data/New/1.json\"  # Name of .json dataset to pass.  Should have 1 tweet per line of file.\n",
    "\n",
    "# output_filename = \"Cleaned_twitter_dataset.json\"  # Name of clean .json file to write to.  Will have 1 clean tweet per line of file.\n",
    "# \n",
    "# list_non_provinces = ['Canada', 'Montréal', 'Subd. A', 'Subd. C', 'Unorganized',\n",
    "#                       'Toronto', 'Vancouver', 'Subd. Y', 'Subd. B', 'Subd. O', '143', 'Subd. D', 'Calgary',\n",
    "#                       'Nouveau-Brunswick', 'Subd. V',\n",
    "#                       'Subd. E']  # List of non-provinces in original tweet to be corrected.\n",
    "# list_sleep_words = [\"bed\", \"sack\", \"insomnia\", \"dodo\", \"zzz\", \"siesta\", \"tired\", \"nosleep\",\n",
    "#                     \"cantsleep\", \"rest\", \"asleep\", \"slept\", \"sleeping\", \"sleepy\",\n",
    "#                     \"ambien\", \"zolpidem\", \"lunesta\", \"intermezzo\", \"trazadone\", \"eszopiclone\",\n",
    "#                     \"zaleplon\"]  # List of sleep words to check for.\n",
    "# list_sleep_bigrams = [[\"pass\", \"out\"], [\"get\", \"up\"], [\"wake\", \"up\"],\n",
    "#                       [\"power\", \"nap\"]]  # List of sleep bigrams to compare tweet text to.\n",
    "# list_stress_words = [\"heart\", \"control\", \"depression\", \"disease\", \"study\", \"studies\", \"life\"\n",
    "#                                                                                       \"stressor\", \"body\", \"stress\",\n",
    "#                      \"anxiety\", \"health\", \"feel\", \"pain\",\n",
    "#                      \"social\", \"stressors\", \"pressure\", \"work\", \"risk\", \"stressful\", \"busy\",\n",
    "#                      \"depressed\", \"nervous\", \"university\", \"cancer\", \"marry\", \"wedding\", \"bride\",\n",
    "#                      \"income\", \"salary\", \"rent\", \"hospital\", \"sick\", \"school\", \"holiday\", \"finals\",\n",
    "#                      \"born\", \"baby\", \"life\", \"fired\", \"job\", \"lose\", \"cold\", \"war\", \"quarrel\", \"argue\"\n",
    "#                                                                                                \"question\", \"blame\",\n",
    "#                      \"afraid\", \"baby\", \"pregnant\", \"mother-to-be\", \"revise\",\n",
    "#                      \"habits\", \"smoke\", \"drink\", \"pass\", \"away\", \"RIP\", \"divorce\", \"ex-wife\", \"cry\",\n",
    "#                      \"surgery\"]  # List of stress keywords to compare tweet text to.\n",
    "# \n",
    "# with open(filename, 'r', encoding='utf8', errors='ignore') as f:  # Open input file\n",
    "#     with open(output_filename, 'w', encoding='utf8') as outputFile:  # Open output file\n",
    "#         for (i, line) in enumerate(f, 1):  # Iterate through each line in input file.\n",
    "#             tweet_dict = json.loads(line)  # Load the .json object into a dictionary.\n",
    "#             if filterTweet(tweet_dict, \"en\", \"Canada\"):  # Filter the tweet.\n",
    "#                 new_tweet = FilteredTweet(tweet_dict)  # Create a new filtered tweet.\n",
    "#                 if new_tweet.place_province is None or new_tweet.place_province in list_non_provinces:  # Correct the province if necessary\n",
    "#                     new_tweet.setProvince()\n",
    "#                 new_tweet = province_filter(new_tweet)  # Check that province has been corrected.\n",
    "#                 if new_tweet is not None:\n",
    "#                     if (checkSleepWords(new_tweet, list_sleep_words, list_sleep_bigrams)) \\\n",
    "#                             or (\n",
    "#                             checkStressWords(new_tweet, list_stress_words)):  # Check for sleep/stress keywords/bigrams.\n",
    "#                         new_tweet.increment()  # Set the index.\n",
    "#                         new_tweet_json = json.dumps(\n",
    "#                             new_tweet.__dict__)  # Create a new json object from the clean tweet.\n",
    "#                         clean_tweet_json = json.dumps(new_tweet.__dict__, indent=4)\n",
    "#                         print(clean_tweet_json)\n",
    "#                         outputFile.write(new_tweet_json)  # Write the new json object to the output file.\n",
    "#                         outputFile.write('\\n')  # Write a newline to separate tweets in output file.\n",
    "#                         print(\"Current tweets written:\", FilteredTweet.counter, \", Tweets evaluated:\", i)\n",
    "#             # if i == 1000000:\n",
    "#             #     break\n",
    "# \n",
    "# print(\"****************************************\")\n",
    "# print(\"Total clean tweets written: \", FilteredTweet.counter)\n",
    "# print(\"Total tweets evaluated: \", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
